% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{standalone}
\usepackage{color}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}


\begin{document}
%
\title{Admission Handler}

\author{Group 18: Jan Haas \and Simon Hauser \and Samuel Kenworthy}

\institute{}
%
\maketitle              % typeset the header of the contribution

\section{Introduction}
This project implements a distributed admission system for large events and venues with limited capacity.
The stream of visitors entering or exiting the venue will be monitored in real-time via devices at each entrance.
Live feedback will be provided to each device, enabling organizers to stop admission when the maximum capacity of the venue has been reached.

As the world starts to reopen after the pandemic and public events become possible again, systems are required to handle the keeping of safe capacities, in-line with changing guidelines and laws.
Pre-selling tickets is one possibility of managing these limits, yet events such as Christmas markets, beer gardens and public concerts live off spontaneous visits.
This project will enable the simple monitoring and managing of such streams of visitors, by providing a dynamically scalable system.

\section{Requirements Analysis}

\input{mrequirement_analysis}

\newpage
\section{Architecture}
%The system is modeled in a single network with at least a machine running multiple clients and at least two running some servers each.
%Both participant types will be written in Python.\\
\begin{figure}
\includegraphics[width=\textwidth]{Architecture_Diagram_new.png}
\end{figure}
\subsection{Demo Setup}
For the demo, we will run several servers and clients on 2+ laptops to ensure a realistic scenario.
Additionally, we implemented both a simple GUI wrapper that can interact with a single client, as well as a demo observer that summarizes the current state of the system.
\section{Implementation Details}
We decided to complete the project using purely Python.
\input{group_management}

\input{leader_election}

\input{multicast}

\input{byzantine}

\input{client}

\input{monitoring}
\newpage
\section{Code}
We will have sent invitations to our private code repository on github by the time this report is read.

\section{Discussion and Conclusion}
Working on this project made clear that even a very simple application requires a large amount of effort before it can be safely run in a distributed way, despite the aspect of security being ignored.
\\\\
From basic functionality to algorithms used for voting, finding a consensus and ensuring data integrity, additional considerations had to be made for cases such as members or message-recipients becoming unreachable mid-process. Which of these issues to handle had how to handle them had to be made based on severeness of impact on the system, as covering all of them would have not been possible in the given time.
Frequently, new edge cases would be found during tests and dealt with  before making sure to reproduce them correctly during later tests to ensure they had been fixed.
Additionally, locating a fault often proved more complex than when dealing with a local or monolithic system, often requiring to log all messages received by the different system components and backtrack to exactly what went wrong in the first place.

Future considerations would include finding an alternate solution to using heartbeats. As the group view relies heavily on (missed) heart beats, it can take a noticeable amount of time for the system to update depending on the heartbeat-interval.
This also means that at times, multicast messaging stalls until the system is sorted out again. The system is eventually-consistent, nevertheless, how well suited this is for a real-time application is debatable.
Additionally, tests with eight or more servers clearly showed the limitations of the byzantine algorithm, as its exponential message complexity flooded the channel, also stalling the system.
In general, not much consideration was given to message complexity, as it was not the objective of this project. However, it did show the importance of complexity and efficient algorithms within such a system, as the above implementation quickly brought our consumer-grade hardware to a sweat.
\\\\
All in all, the experience gained from this project should help in making better decisions not only when developing system components actually concerned with distribution, but also when developing components that will run in a distributed system and should thus be ready to deal with faults not present in a monolithic system.
\end{document}
